{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5\n",
    "\n",
    "**Introduction:**\n",
    "#### Regularisation involves shrinking the estimated coefficients towards zero relative to least  square estimates. By shrinking the estimated coefficients we substantially reduce the variance at the cost of negligible increase in bias.\n",
    "#### There are two shrinkage techniques: Ridge regression and LASSO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(plotly): there is no package called 'plotly'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(plotly): there is no package called 'plotly'\nTraceback:\n",
      "1. library(plotly)"
     ]
    }
   ],
   "source": [
    "rm(list = ls())\n",
    "set.seed(123)\n",
    "\n",
    "##############################Required packages#################################\n",
    "\n",
    "library(glmnet)\n",
    "library(ggplot2)\n",
    "library(plotly)\n",
    "library(mvtnorm)\n",
    "library(ggpubr)\n",
    "require(reshape2)\n",
    "library(patchwork)\n",
    "\n",
    "####################################Given Parameters and DGP####################\n",
    "\n",
    "n <- 100\n",
    "p <- 3\n",
    "beta <- c(0.5, 0.5, -0.5)\n",
    "eps_sd<- sqrt(10)\n",
    "x_mu <- rep(0,p)\n",
    "var <- matrix(c(16,0,1.38, 0,116,0.04, 1.38,0.04,18), ncol=3)\n",
    "\n",
    "# DGP without intercept\n",
    "eps <- rnorm(n, 0, eps_sd)\n",
    "x <- rmvnorm(n, x_mu, var) \n",
    "y <- x%*%beta + eps\n",
    "\n",
    "# Test sample and training sample\n",
    "dt = sort(sample(nrow(x), nrow(x)*0.5))\n",
    "x_train <- x[dt, ]\n",
    "x_test  <- x[-dt,]\n",
    "y_train <- y[dt, ]\n",
    "y_test  <- y[-dt,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1, a**\n",
    "\n",
    "Write a function to calculate ridge regression estimator in closed form:\n",
    "\n",
    "#### Ridge regression: Ridge regression estimates are values that minimizes \n",
    "\n",
    "$ \\hat{\\beta}_{ridge} = \\arg \\min_{b_0, ..., b_K} \\sum_{i=1}^{n} ( Y_i - b_0 - X_{i1}b_1 - ... -  X_{ik}b_k )^2 + \\lambda \\sum_{k = 0}^{K} b_k^2  \\\\\n",
    " = \\Bigg( \\sum_{i=1}^{n} X_i X_{i}^{'} + \\lambda I_{ (K+1) \\times (K+1) } \\Bigg ) ^{-1}  \\sum_{i = 1}^{n} X_iY_i $\n",
    "\n",
    "for $Y_i = \\beta_0 + X_{i1} \\beta_1 +  X_{i2} \\beta_2 + ...+ X_{ik} \\beta_k + U_i $, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######################## QUESTION A ############################################\n",
    "\n",
    "func_ridge <- function(lambda, x, y){\n",
    "  \n",
    "  if(sd(x[,1])==0){\n",
    "    x <- cbind(x[,1], scale(x[,2:3]))\n",
    "  } else {\n",
    "    x <- scale(x)\n",
    "    y <- scale(y)\n",
    "  }\n",
    "  \n",
    "  b_ridge <-solve((t(x)%*%x+lambda*diag(p)))%*% t(x)%*%y\n",
    "  \n",
    "  return(b_ridge)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1, b**\n",
    "\n",
    "Specify a grid of 100 lambda values and find ridge reg coef and plot the coef paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#############################Question B ########################################\n",
    "\n",
    "m <- 100\n",
    "lambda <- seq(0 , 500, length = m)\n",
    "store_ridge <- matrix(NaN, m, p)\n",
    "\n",
    "for(i in 1:m){\n",
    "  store_ridge[i,] <- func_ridge(lambda[i], x=x_train, y=y_train)\n",
    "}\n",
    "\n",
    "round(head(store_ridge), 3)\n",
    "round(tail(store_ridge), 3)\n",
    "\n",
    "###Plot 1 - Coefficient paths \n",
    "Result1 <- data.frame(cbind(store_ridge,lambda, ld = round(lambda, digits=0)))\n",
    "options(warn = -1)\n",
    "graph1 <- ggplot(data = Result1, aes(x = lambda)) +\n",
    "  geom_hline(yintercept = 0, linetype=\"dashed\")+\n",
    "  geom_point(aes(y = store_ridge[,1], colour = \"beta1\", frame = ld)) + geom_line(aes(y =store_ridge[,1], colour = \"beta1\")) +\n",
    "  geom_point(aes(y = store_ridge[,2], colour = 'beta2', frame = ld)) + geom_line(aes(y = store_ridge[,2], colour = \"beta2\")) +\n",
    "  geom_point(aes(y = store_ridge[,3], colour = \"beta3\", frame = ld)) + geom_line(aes(y = store_ridge[,3], colour = \"beta3\")) +\n",
    "  labs(title = 'Path of coefficients', x = 'Lambda', y = 'Standardized Coefficients') +\n",
    "  scale_colour_manual(\"coefficients\", breaks = c(\"beta1\", \"beta2\", \"beta3\"), values = c( \"red\", \"blue\", \"green\")) +  \n",
    "  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "        panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "ggplotly(graph1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1, c** \n",
    "\n",
    "Generate a test sample of same size and calc prediction and training error for each  lambda and OLS estimate. Show results in graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "################################### Question C ##################################\n",
    "\n",
    "store_errors <- matrix(NaN, m, 3)\n",
    "\n",
    "for(i in 1:m){\n",
    "  \n",
    "  store_errors[i,2] <- mean((y_test - x_test %*% store_ridge[i,])^2)\n",
    "  store_errors[i,3] <- mean((y_train - x_train %*% store_ridge[i,])^2)\n",
    "  \n",
    "}\n",
    "store_errors <- data.frame(\"Lambda\" = lambda, \"Prediction Error\" = store_errors[,2], \"Training Error\" = store_errors[,3])\n",
    "round(head(store_errors), 3)\n",
    "round(tail(store_errors), 3)\n",
    "\n",
    "# Plot 2 - Prediction and training error\n",
    "\n",
    "Result_errors <- data.frame(cbind(store_errors, lambda, ld = round(lambda, digits=0)))\n",
    "options(warn= -1)\n",
    "graph_error <- ggplot(data = Result_errors, aes(x = lambda)) +\n",
    "  geom_point(aes(y =store_errors[,2], colour = \"Prediction error\", frame = ld)) + geom_line(aes(y =store_errors[,2], colour = \"Prediction error\")) +\n",
    "  geom_point(aes(y =store_errors[,3], colour = 'Training error', frame = ld)) + geom_line(aes(y =store_errors[,3], colour = \"Training error\")) +\n",
    "  labs(title = 'Path of Training and Test errors', x = 'Lambda', y = 'Errors') +\n",
    "  scale_colour_manual(\"coefficients\", breaks = c(\"Prediction error\", \"Training error\"),\n",
    "                      values = c( \"red\", \"blue\"))   +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "                                                             panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "ggplotly(graph_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1, d** \n",
    "\n",
    "Replace one of the covariates to -10. Show the coefficient paths graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "###################################### Question D ###############################\n",
    "\n",
    "\n",
    "#DGP with a constant\n",
    "x_cons   <- cbind(rep(-10,n), x[,2:3])\n",
    "y_cons   <- x_cons%*%beta + eps\n",
    "\n",
    "#Training data\n",
    "x_train_cons <- x_cons[dt, ]\n",
    "y_train_cons <- y_cons[dt, ]\n",
    "\n",
    "store_ridge_cons <- matrix(NaN, m, p)\n",
    "\n",
    "for(i in 1:m){\n",
    "  store_ridge_cons[i,] <- func_ridge(lambda[i], x=x_train_cons, y=y_train_cons)\n",
    "}\n",
    "\n",
    "round(head(store_ridge), 4)\n",
    "round(head(store_ridge_cons), 4)\n",
    "\n",
    "\n",
    "###################### Plot 3 - COefficient path_ new and old ###################\n",
    "Result2 <- data.frame(cbind(store_ridge_cons, lambda))\n",
    "graph2 <- ggplot(data = Result2, aes(x = lambda)) +\n",
    "  geom_hline(yintercept = 0, linetype=\"dashed\")+\n",
    "  geom_line(aes(y =store_ridge[,2], colour = \"beta2\")) + geom_line(aes(y =store_ridge_cons[,2], colour = \"beta2_new\")) +\n",
    "  labs(title = 'Path of coefficients', x = 'Lambda', y = 'Standardized Coefficients') +\n",
    "  scale_colour_manual(\"coefficients\", breaks = c(\"beta2\", \"beta2_new\"),\n",
    "                      values = c( \"blue\", \"red\"))   +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "                                                             panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "graph3 <- ggplot(data = Result2, aes(x = lambda)) +\n",
    "  geom_hline(yintercept = 0, linetype=\"dashed\")+\n",
    "  geom_line(aes(y =store_ridge[,1], colour = \"beta1\")) + geom_line(aes(y =store_ridge_cons[,1], colour = \"beta1_new\")) +\n",
    "  labs(title = 'Path of coefficients', x = 'Lambda', y = 'Standardized Coefficients') +\n",
    "  scale_colour_manual(\"coefficients\", breaks = c(\"beta1\", \"beta1_new\"),\n",
    "                      values = c( \"blue\", \"red\"))   +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "                                                             panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "graph4 <- ggplot(data = Result2, aes(x = lambda)) +\n",
    "  geom_hline(yintercept = 0, linetype=\"dashed\")+\n",
    "  geom_line(aes(y =store_ridge[,3], colour = \"beta3\")) + geom_line(aes(y =store_ridge_cons[,3], colour = \"beta3_new\")) +\n",
    "  labs(title = 'Path of coefficients', x = 'Lambda', y = 'Standardized Coefficients') +\n",
    "  scale_colour_manual(\"coefficients\", breaks = c(\"beta3\", \"beta3_new\"),\n",
    "                      values = c( \"blue\", \"red\"))   +  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "                                                             panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "figure <- ggarrange(graph2, graph3, graph4, ncol =2, nrow = 2)\n",
    "figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1, e** \n",
    "\n",
    "Use glmnet package and determine optimal lambda from 10-fold cross-validation error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "################################## Question e ####################################\n",
    "\n",
    "cross <- cv.glmnet(x_train, y_train, alpha=0, nfolds=10)\n",
    "bestlam = cross$lambda.min\n",
    "bestlam \n",
    "\n",
    "###### Plot 4 Mean Cross Validation Error\n",
    "Result4 <- data.frame(cbind(cross$lambda, cross$cvm, ld = round(cross$lambda, digits=0)))\n",
    "options(warn = -1)\n",
    "graph7 <- ggplot(data = Result4, aes(x = cross$lambda)) +\n",
    "  geom_vline(xintercept = bestlam, linetype=\"dashed\") +\n",
    "  geom_point(aes(y = cross$cvm, colour = \"cvm\", frame = ld)) + geom_line(aes(y = cross$cvm, colour = \"cvm\")) +\n",
    "  labs(title = 'Path of Mean CV error', x = 'Lambda', y = 'CV error') +\n",
    "  scale_colour_manual(\"\", breaks =\"cvm\", values = \"red\") +  \n",
    "  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n",
    "        panel.background = element_blank(), plot.title = element_text(hjust = 0.5), axis.line = element_line(colour = \"black\"))\n",
    "\n",
    "ggplotly(graph7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2, a** \n",
    "\n",
    "Compare the performance of the OLS estimator and the ridge regression estimator in terms of the prediction\n",
    "error for 100 simulation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rm(list = ls())\n",
    "\n",
    "# Set sample size and number of simulations\n",
    "n = 100\n",
    "n_sims = 100\n",
    "\n",
    "# Set number of regressors (p), true Parameters (beta) and variance of error\n",
    "p = 3\n",
    "beta = c(0.5,0.5,-0.5)\n",
    "esp_sd = 10**0.5\n",
    "esp = rnorm(n,0,esp_sd)\n",
    "\n",
    "#Define var-cov matrix\n",
    "\n",
    "cov_mat <- function(m) {\n",
    "    A <- matrix(runif(m^2)*2-1, ncol=m) \n",
    "    Sigma <- t(A) %*% A\n",
    "  return(Sigma)\n",
    "  }\n",
    "\n",
    "# Set out-of-sample Data\n",
    "\n",
    "x_mat_oos = rmvnorm(n, rep(0,p), cov_mat(p))\n",
    "Y_oos = x_mat_oos%*%beta + esp\n",
    "\n",
    "predict_error = data.frame(matrix(ncol = 2, nrow = 0)) \n",
    "colnames(predict_error) = c(\"OLS\", \"Ridge\")\n",
    "\n",
    "# Conduct the simulation for 100 rounds, in each calculate estimated parameters\n",
    "\n",
    "for (i_sims in 1:n_sims) {\n",
    "  x_mat = rmvnorm(n, rep(0,p), cov_mat(p))\n",
    "  Y = x_mat%*%beta + esp\n",
    "\n",
    "  # Determine the optimal value of lambda using cross validation\n",
    "\n",
    "  lambda = cv.glmnet(x_mat, Y, alpha = 0, nfolds = 10)$lambda.min\n",
    "\n",
    "  # Estimate coefficients for Regression and Ridge \n",
    "\n",
    "  coefs_est_ridge = c(solve(t(x_mat) %*% x_mat + lambda*diag(p)) %*% t(x_mat) %*% Y)\n",
    "  coefs_est_ols = c(solve(t(x_mat) %*% x_mat) %*% t(x_mat) %*% Y)\n",
    "\n",
    "  # Now calculate the  prediction Error \n",
    "  \n",
    "  res_predict_ols = Y_oos - x_mat_oos%*%coefs_est_ols\n",
    "  s2_predict_ols = sum(res_predict_ols^2)/(n) \n",
    "  res_predict_ridge = Y_oos - x_mat_oos%*%coefs_est_ridge\n",
    "  s2_predict_ridge = sum(res_predict_ridge^2)/(n) \n",
    "\n",
    "  #Update the data frame of predicted errors \n",
    "\n",
    "  predict_error[i_sims,] = c(s2_predict_ols, s2_predict_ridge)\n",
    "\n",
    "}\n",
    "\n",
    "#Mean Prediction Error \n",
    "\n",
    "average_pred_error = colMeans(predict_error[sapply(predict_error, is.numeric)]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2, b** \n",
    "\n",
    "Propose a manipulation of the dgp that will improve the ridge regression performance relative to the OLS\n",
    "estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rm(list = ls())\n",
    "\n",
    "# Set correlated variance and covariance matrix\n",
    "\n",
    "cov_mat_corr <- function(m) {\n",
    "    A <- matrix(runif(m^2)*2-1, ncol=m) \n",
    "    Sigma <- t(A) %*% A\n",
    "  return(Sigma)\n",
    "}\n",
    "\n",
    "# Set uncorrelated variance and covariance matrix\n",
    "\n",
    "cov_mat_uncorr <- function(m) {\n",
    "  return(diag(m))\n",
    "}\n",
    "\n",
    "\n",
    "# The function ad_ridge_ols takes number of simulations, number of observations, \n",
    "# number of features, variance of error, and \n",
    "# returns prediction error disaggregated by OLS and Ridge.  \n",
    "\n",
    "ad_ridge_ols <- function(n_sims,n,p,var,covvarmat){\n",
    "\n",
    "  beta = runif(p, min= -1, max=1)\n",
    "  esp_sd = var**0.5\n",
    "  predict_error = data.frame(matrix(ncol = 2, nrow = 0)) \n",
    " \n",
    "# Conduct Simulation for 100 rounds \n",
    "\n",
    "  for (i_sims in 1:n_sims) {\n",
    "    esp = rnorm(n,0,esp_sd)\n",
    "    x_mat = rmvnorm(n, rep(0,p), covvarmat)\n",
    "    Y = x_mat%*%beta + esp\n",
    "    \n",
    "  # Draw Test Sample using identical distribution \n",
    "\n",
    "    x_mat_oos = rmvnorm(n, rep(0,p), covvarmat)\n",
    "    Y_oos = x_mat_oos%*%beta + esp\n",
    "\n",
    "  # Use the optimal Lambda given by  10 fold cross-validation \n",
    "\n",
    "    lambda = cv.glmnet(x_mat, Y, alpha = 0, nfolds = 10)$lambda.min\n",
    "\n",
    "  # Estimating OLS and Ridge Coefficients\n",
    "    \n",
    "    coefs_est_ridge = c(solve(t(x_mat) %*% x_mat + lambda*diag(p)) %*% t(x_mat) %*% Y)\n",
    "    coefs_est_ols = c(solve(t(x_mat) %*% x_mat) %*% t(x_mat) %*% Y)\n",
    "\n",
    "    # Prediction Error \n",
    "    res_predict_ols = Y_oos - x_mat_oos%*%coefs_est_ols\n",
    "    s2_predict_ols = sum(res_predict_ols^2)/(n) \n",
    "    res_predict_ridge = Y_oos - x_mat_oos%*%coefs_est_ridge\n",
    "    s2_predict_ridge = sum(res_predict_ridge^2)/(n) \n",
    "    # Update the values \n",
    "    predict_error[i_sims,] = c(s2_predict_ols, s2_predict_ridge)\n",
    "    }\n",
    "  return(colMeans(predict_error[sapply(predict_error, is.numeric)]))\n",
    "}\n",
    "\n",
    "# The function takes max number of features and maps prediction errors when \n",
    "# number of features range from 3 to the set max number of features.  \n",
    "\n",
    "features_ridge_ols <- function(p){ \n",
    "  feature_pred_error = data.frame(matrix(ncol = 3, nrow = 0)) \n",
    "  colnames(feature_pred_error) = c(\"Features\", \"OLS\", \"Ridge\")\n",
    "  feature_l_seq = seq(3, p, length = 30) \n",
    "  for (i_p in floor(feature_l_seq)) {\n",
    "    feature_pred_error[i_p,] =  c(i_p, ad_ridge_ols(n_sims = 100, n = 100, i_p, var = 10, cov_mat_uncorr(i_p)))\n",
    "    }\n",
    "  return(feature_pred_error[complete.cases(feature_pred_error),])\n",
    "}\n",
    "\n",
    "prediction_errors = features_ridge_ols(90)\n",
    "\n",
    "# Plot when features are changed from 3 to 90.\n",
    "\n",
    "df_prediction_errors = melt(prediction_errors ,  id.vars = 'Features', variable.name = 'Types')\n",
    "plot_prediction_errors = ggplot(df_prediction_errors, aes(Features,value)) + \n",
    "                         geom_line(aes(colour = Types)) + \n",
    "                         ylab(\"Prediction Erros\") + \n",
    "                         ggtitle(\"Evolution of prediction errors with increase in number of predictors\") + \n",
    "                         xlab(\" Number of Features\") \n",
    "\n",
    "print(plot_prediction_errors)\n",
    "\n",
    "# Change the variance of error, take only 10 values in the range. \n",
    "\n",
    "var_ridge_ols <- function(v){ \n",
    "  var_pred_error = data.frame(matrix(ncol = 3, nrow = 0)) \n",
    "  var_all = seq(from = 1, to = v,  length = 10)\n",
    "  p = 25\n",
    "  colnames(var_pred_error) = c(\"Variance\", \"OLS\", \"Ridge\")\n",
    "  for (i_v in var_all) {\n",
    "    var_pred_error[i_v,] =  c(i_v, ad_ridge_ols(n_sims = 100, n = 100, p, var = i_v,cov_mat_uncorr(p)))\n",
    "    }\n",
    "  return(var_pred_error[complete.cases(var_pred_error),])\n",
    "}\n",
    "\n",
    "# We allow variance to range from 1 to 100, and work with 10 values in between\n",
    "\n",
    "change_variance_errors = var_ridge_ols(100)\n",
    "\n",
    "#  Now Draw the Graphs\n",
    "\n",
    "df_change_variance_errors = melt(change_variance_errors ,  id.vars = 'Variance', variable.name = 'Types')\n",
    "plot_change_variance_errors = ggplot(df_change_variance_errors, aes(Variance,value)) + \n",
    "                         geom_line(aes(colour = Types)) + \n",
    "                         ylab(\"Prediction Errors\") + \n",
    "                         ggtitle(\"Evolution of prediction errors with increase in Variance for error term\") + \n",
    "                         xlab(\"Variance of Error Term\") \n",
    "\n",
    "print(plot_change_variance_errors)\n",
    "\n",
    "#  Now Test the third data generation Process, which looks into\n",
    "#  effects of changing the variance and covariance matrix\n",
    "#  use n = 100, n_sims = 100, p = 90\n",
    "\n",
    "iid_ridge_ols <- function(p){ \n",
    "  iid_pred_error = data.frame(matrix(ncol = 3, nrow = 0)) \n",
    "  colnames(iid_pred_error) = c(\"Features\", \"OLS\", \"Ridge\")\n",
    "  feature_l_seq = seq(3, p, length = 30) \n",
    "  for (i_p in floor(feature_l_seq)) {\n",
    "    iid_pred_error[i_p,] =  c(i_p, ad_ridge_ols(n_sims = 100, n = 100, i_p, var = 10, cov_mat_uncorr(i_p)))\n",
    "    }\n",
    "  return(iid_pred_error[complete.cases(iid_pred_error),])\n",
    "}\n",
    "\n",
    "iid_prediction_errors = iid_ridge_ols(90)\n",
    "\n",
    "\n",
    "# The following function predicts error when x-variables are correlated. \n",
    "\n",
    "crr_ridge_ols <- function(p){ \n",
    "  cid_pred_error = data.frame(matrix(ncol = 3, nrow = 0)) \n",
    "  colnames(cid_pred_error) = c(\"Features\", \"OLS\", \"Ridge\")\n",
    "  feature_l_seq = seq(3, p, length = 30) \n",
    "  for (i_p in floor(feature_l_seq)) {\n",
    "    cid_pred_error[i_p,] =  c(i_p, ad_ridge_ols(n_sims = 100, n = 100, i_p, var = 10,cov_mat_corr(i_p)))\n",
    "    }\n",
    "  return(cid_pred_error[complete.cases(cid_pred_error),])\n",
    "}\n",
    "\n",
    "cr_prediction_errors = crr_ridge_ols(90)\n",
    "\n",
    "#  Graphs g_corr refers to coorrelated and g_iid for uncoorelated x-variables. In both cases, \n",
    "#  we map prediction error with number of features. \n",
    "\n",
    "\n",
    "df_g_corr = melt(cr_prediction_errors ,  id.vars = 'Features', variable.name = 'Types')\n",
    "plot_g_corr = ggplot(df_g_corr, aes(Features,value)) + \n",
    "                         geom_line(aes(colour = Types)) + \n",
    "                         ylab(\"Prediction Errors\") + \n",
    "                         ggtitle(\"Evolution of prediction errors with increase in  number of correlated predictors\") + \n",
    "                         xlab(\"# Number of Features\") \n",
    "\n",
    "print(plot_g_corr)\n",
    "\n",
    "\n",
    "df_g_uncorr = melt(iid_prediction_errors ,  id.vars = 'Features', variable.name = 'Types')\n",
    "plot_g_uncorr = ggplot(df_g_uncorr, aes(Features,value)) + \n",
    "                         geom_line(aes(colour = Types)) + \n",
    "                         ylab(\"Prediction Errors\") + \n",
    "                         ggtitle(\"Evolution of prediction errors with increase in  number of uncorrelated predictors\") + \n",
    "                         xlab(\"# Number of Features\") \n",
    "\n",
    "print(plot_g_uncorr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
